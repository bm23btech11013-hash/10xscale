{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb sentence-transformers google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc164e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyApj19v7hlquafvI6nJ7qK7gkVrJR1siQ4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Any, Optional\n",
    "import datetime as dt\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import CrossEncoder\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"                     # .txt policy files\n",
    "CHROMA_PATH = \"chroma_store\"          # local Chroma directory\n",
    "\n",
    "# Cache files\n",
    "METADATA_CACHE_PATH = \"metadata_cache.json\"\n",
    "ANSWER_CACHE_PATH   = \"answer_cache.json\"\n",
    "\n",
    "COLLECTION_NAME = \"nebula_policies\"\n",
    "\n",
    "EMBED_MODEL   = \"all-MiniLM-L6-v2\"\n",
    "RERANK_MODEL  = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "GEMINI_MODEL  = \"gemini-2.5-flash\"\n",
    "\n",
    "\n",
    "\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\" Please set GOOGLE_API_KEY before running this script.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "#  SMALL UTILS: LOAD/SAVE JSON CACHES\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_json(path: str, data: dict):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "# In-memory caches (backed by JSON files)\n",
    "metadata_cache = load_json(METADATA_CACHE_PATH)  # key: filename\n",
    "answer_cache   = load_json(ANSWER_CACHE_PATH)    # key: md5(query)\n",
    "\n",
    "\n",
    "\n",
    "#  BASIC HELPERS\n",
    "\n",
    "\n",
    "def clean_metadata(meta: dict) -> dict:\n",
    "    return {k: (\"\" if v is None else v) for k, v in meta.items()}\n",
    "\n",
    "\n",
    "def parse_date(d: str) -> Optional[dt.date]:\n",
    "    if not d:\n",
    "        return None\n",
    "    d = d.strip()\n",
    "    for fmt in (\"%Y-%m-%d\", \"%d-%m-%Y\", \"%Y/%m/%d\", \"%d/%m/%Y\"):\n",
    "        try:\n",
    "            return dt.datetime.strptime(d, fmt).date()\n",
    "        except Exception:\n",
    "            continue\n",
    "    try:\n",
    "        return dt.date.fromisoformat(d)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_version(v: str) -> float:\n",
    "    if not v:\n",
    "        return 0.0\n",
    "    v = v.lower().strip()\n",
    "    if v.startswith(\"v\"):\n",
    "        v = v[1:]\n",
    "    try:\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "#  METADATA EXTRACTION (LLM) WITH CACHE\n",
    "\n",
    "\n",
    "def extract_metadata_llm(text: str, filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs ONCE per doc thanks to metadata_cache.\n",
    "    \"\"\"\n",
    "\n",
    "    if filename in metadata_cache:\n",
    "        return metadata_cache[filename]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an HR policy metadata extractor.\n",
    "\n",
    "Return ONLY valid JSON (no explanations).\n",
    "\n",
    "Extract:\n",
    "- audience_scope: one of\n",
    "  [\"interns\",\"full_time_employees\",\"managers\",\"contractors\",\"company_wide\",\"all_employees\",\"other\"]\n",
    "- policy_type: short snake_case like\n",
    "  \"remote_work\", \"vacation\", \"sick_leave\", \"benefits\", \"travel\", \"code_of_conduct\", etc.\n",
    "- effective_date: ISO \"YYYY-MM-DD\" if explicitly stated, else \"\".\n",
    "- version: like \"v1\", \"v2\", \"v2.1\" if present, else \"\".\n",
    "- specificity_level: one of [\"role_specific\",\"team_specific\",\"company_wide\",\"unclear\"].\n",
    "- override_notes: short string explaining if this document updates/overrides earlier policies (or \"\" if not clear).\n",
    "\n",
    "Document filename: {filename}\n",
    "\n",
    "Document content (trimmed):\n",
    "\\\"\\\"\\\"{text[:4000]}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    try:\n",
    "        meta = json.loads(resp.text)\n",
    "    except Exception:\n",
    "        meta = {}\n",
    "\n",
    "    final = {\n",
    "        \"audience_scope\":   meta.get(\"audience_scope\", \"other\"),\n",
    "        \"policy_type\":      meta.get(\"policy_type\", \"uncategorized\"),\n",
    "        \"effective_date\":   meta.get(\"effective_date\", \"\"),\n",
    "        \"version\":          meta.get(\"version\", \"\"),\n",
    "        \"specificity_level\":meta.get(\"specificity_level\", \"unclear\"),\n",
    "        \"override_notes\":   meta.get(\"override_notes\", \"\"),\n",
    "    }\n",
    "\n",
    "    metadata_cache[filename] = final\n",
    "    save_json(METADATA_CACHE_PATH, metadata_cache)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "# QUERY CONTEXT (user_role + policy_type)\n",
    "\n",
    "\n",
    "def extract_query_context(query: str) -> Dict[str, Any]:\n",
    "    prompt = f\"\"\"\n",
    "You are a classifier for HR questions.\n",
    "\n",
    "Return ONLY JSON (no explanation) with:\n",
    "- user_role: one of [\"intern\",\"full_time_employee\",\"manager\",\"contractor\",\"unknown\"]\n",
    "- policy_type: short snake_case label like \"remote_work\", \"vacation\", \"sick_leave\", etc.\n",
    "\n",
    "User question:\n",
    "\\\"\\\"\\\"{query}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    try:\n",
    "        ctx = json.loads(resp.text)\n",
    "    except Exception:\n",
    "        ctx = {}\n",
    "\n",
    "    return {\n",
    "        \"user_role\":  ctx.get(\"user_role\", \"unknown\"),\n",
    "        \"policy_type\":ctx.get(\"policy_type\", \"uncategorized\"),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#  INIT & INGEST (NO RE-INGEST IF ALREADY DONE)\n",
    "\n",
    "\n",
    "def init_chroma():\n",
    "    client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "    embed_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBED_MODEL\n",
    "    )\n",
    "    return client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embed_fn\n",
    "    )\n",
    "\n",
    "\n",
    "def ingest_all_txt(collection):\n",
    "    \"\"\"\n",
    "    Only ingests if collection is empty.\n",
    "    Saves LLM + Chroma cost on subsequent calls.\n",
    "    \"\"\"\n",
    "    if collection.count() > 0:\n",
    "        return\n",
    "\n",
    "    paths = glob.glob(f\"{DOC_DIR}/*.txt\")\n",
    "    if not paths:\n",
    "        print(\"❌ No .txt files found.\")\n",
    "        return\n",
    "\n",
    "    for path in paths:\n",
    "        doc_id = os.path.basename(path)\n",
    "\n",
    "        # Not strictly needed if count()==0, but safe if reused in future\n",
    "        if collection.get(ids=[doc_id])[\"ids\"]:\n",
    "            continue\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        metadata = extract_metadata_llm(text, doc_id)\n",
    "        safe_meta = clean_metadata({\"source\": doc_id, **metadata})\n",
    "\n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[text],\n",
    "            metadatas=[safe_meta]\n",
    "        )\n",
    "\n",
    "    print(\" Ingestion complete with metadata.\")\n",
    "\n",
    "\n",
    "#  RETRIEVAL\n",
    "\n",
    "\n",
    "def retrieve_candidates(collection, query: str, q_ctx: Dict[str, Any], top_n=50):\n",
    "\n",
    "    where = {}\n",
    "    policy_type = q_ctx.get(\"policy_type\")\n",
    "    if policy_type and policy_type != \"uncategorized\":\n",
    "        where[\"policy_type\"] = policy_type\n",
    "\n",
    "    result = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_n,\n",
    "        where=where or None\n",
    "    )\n",
    "\n",
    "    docs = []\n",
    "    if not result[\"ids\"]:\n",
    "        return docs\n",
    "\n",
    "    for i, doc_id in enumerate(result[\"ids\"][0]):\n",
    "        docs.append({\n",
    "            \"id\": doc_id,\n",
    "            \"text\": result[\"documents\"][0][i],\n",
    "            \"meta\": result[\"metadatas\"][0][i]\n",
    "        })\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "#  RERANK WITH CONFLICT LOGIC\n",
    "\n",
    "\n",
    "reranker = None\n",
    "\n",
    "def rerank_with_conflict_logic(query: str,\n",
    "                               docs: List[Dict],\n",
    "                               q_ctx: Dict[str, Any],\n",
    "                               top_k=3) -> List[Dict[str, Any]]:\n",
    "\n",
    "    global reranker\n",
    "    if reranker is None:\n",
    "        reranker = CrossEncoder(RERANK_MODEL)\n",
    "\n",
    "    user_role = q_ctx.get(\"user_role\", \"unknown\")\n",
    "\n",
    "    pairs = [[query, d[\"text\"]] for d in docs]\n",
    "    base_scores = reranker.predict(pairs)\n",
    "\n",
    "    scored_docs = []\n",
    "    for d, s in zip(docs, base_scores):\n",
    "        meta = d.get(\"meta\", {})\n",
    "        aud  = (meta.get(\"audience_scope\") or \"\").lower()\n",
    "        spec = (meta.get(\"specificity_level\") or \"\").lower()\n",
    "        eff  = parse_date(meta.get(\"effective_date\", \"\"))\n",
    "        ver  = parse_version(meta.get(\"version\", \"\"))\n",
    "\n",
    "        bonus = 0.0\n",
    "\n",
    "        # Role-specific bonus\n",
    "        if user_role != \"unknown\":\n",
    "            if user_role in aud:  # e.g. intern in \"interns\"\n",
    "                bonus += 2.0\n",
    "            elif aud in (\"all_employees\", \"company_wide\"):\n",
    "                bonus += 0.5\n",
    "\n",
    "        # Specificity bonus\n",
    "        if spec == \"role_specific\":\n",
    "            bonus += 0.5\n",
    "        elif spec == \"company_wide\":\n",
    "            bonus += 0.2\n",
    "\n",
    "        # Recency bonus\n",
    "        if eff:\n",
    "            years_since_2000 = (eff - dt.date(2000, 1, 1)).days / 365.0\n",
    "            bonus += 0.02 * years_since_2000\n",
    "\n",
    "        # Version bonus\n",
    "        if ver > 0:\n",
    "            bonus += 0.1 * ver\n",
    "\n",
    "        total_score = float(s) + float(bonus)\n",
    "        scored_docs.append({**d, \"score\": total_score})\n",
    "\n",
    "    scored_docs.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return scored_docs[:top_k]\n",
    "\n",
    "\n",
    "\n",
    "#   RELEVANT SEGMENT EXTRACTION\n",
    "\n",
    "\n",
    "def extract_relevant_segment(query: str,\n",
    "                             text: str,\n",
    "                             max_chars: int = 1200,\n",
    "                             context_margin: int = 400) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Heuristic, no extra LLM calls.\n",
    "    Reduce tokens passed to Gemini while keeping relevant context.\n",
    "    \"\"\"\n",
    "    text_str = text if isinstance(text, str) else str(text)\n",
    "    q_lower = query.lower()\n",
    "\n",
    "    idx = -1\n",
    "    # try to find first occurrence of any query word\n",
    "    for word in q_lower.split():\n",
    "        w = word.strip()\n",
    "        if not w:\n",
    "            continue\n",
    "        pos = text_str.lower().find(w)\n",
    "        if pos != -1:\n",
    "            idx = pos\n",
    "            break\n",
    "\n",
    "    if idx == -1:\n",
    "        # fallback: just return the beginning\n",
    "        return text_str[:max_chars]\n",
    "\n",
    "    start = max(0, idx - context_margin)\n",
    "    end   = min(len(text_str), start + max_chars)\n",
    "    return text_str[start:end]\n",
    "\n",
    "\n",
    "# FINAL REASONING OVER MULTIPLE DOCS \n",
    "\n",
    "\n",
    "def reason_over_docs(query: str,\n",
    "                     docs: List[Dict[str, Any]],\n",
    "                     q_ctx: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Uses relevant segments instead of whole documents to save tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    context_blocks = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        m = d[\"meta\"]\n",
    "        segment = extract_relevant_segment(query, d[\"text\"])\n",
    "\n",
    "        context_blocks.append(\n",
    "            f\"[DOC {i}]\\n\"\n",
    "            f\"source: {m.get('source')}\\n\"\n",
    "            f\"audience_scope: {m.get('audience_scope')}\\n\"\n",
    "            f\"policy_type: {m.get('policy_type')}\\n\"\n",
    "            f\"effective_date: {m.get('effective_date')}\\n\"\n",
    "            f\"version: {m.get('version')}\\n\"\n",
    "            f\"specificity_level: {m.get('specificity_level')}\\n\"\n",
    "            f\"override_notes: {m.get('override_notes')}\\n\"\n",
    "            f\"--- RELEVANT EXCERPT ---\\n\"\n",
    "            f\"{segment}\\n\"\n",
    "            f\"--- END EXCERPT ---\\n\"\n",
    "        )\n",
    "\n",
    "    context_str = \"\\n\\n\".join(context_blocks)\n",
    "    user_role = q_ctx.get(\"user_role\", \"unknown\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an HR policy assistant.\n",
    "\n",
    "User role (from query): {user_role}\n",
    "User question:\n",
    "\\\"\\\"\\\"{query}\\\"\\\"\\\"\n",
    "\n",
    "You are given multiple policy document excerpts (not full text). They may conflict.\n",
    "\n",
    "Excerpts:\n",
    "{context_str}\n",
    "\n",
    "INSTRUCTIONS (VERY IMPORTANT):\n",
    "1. Use ONLY the provided excerpts. Do not invent rules.\n",
    "2. If there is a document that is specific to the user's role (e.g. interns),\n",
    "   that role-specific document OVERRIDES more general documents for that user.\n",
    "3. If multiple documents apply to the same role, prefer:\n",
    "   - the one with the most recent effective_date, or\n",
    "   - the higher version (e.g. v2 > v1), or\n",
    "   - explicit override_notes that say it updates/replaces earlier policies.\n",
    "4. Your answer MUST:\n",
    "   - Directly answer the user's question in 1–3 sentences.\n",
    "   - Briefly explain why (mentioning the conflict resolution in 1–2 sentences).\n",
    "   - Explicitly mention which document(s) provided the final ruling, by filename.\n",
    "\n",
    "FORMAT:\n",
    "Final Answer: <very short direct answer dont give reasoning>\n",
    "Sources: <comma-separated list of source filenames>\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL)\n",
    "    resp = model.generate_content(prompt)\n",
    "    return resp.text.strip()\n",
    "\n",
    "\n",
    "\n",
    "# ANSWER CACHE\n",
    "\n",
    "\n",
    "def cached_answer(query: str) -> Optional[str]:\n",
    "    key = hashlib.md5(query.encode(\"utf-8\")).hexdigest()\n",
    "    return answer_cache.get(key)\n",
    "\n",
    "def store_answer(query: str, answer: str):\n",
    "    key = hashlib.md5(query.encode(\"utf-8\")).hexdigest()\n",
    "    answer_cache[key] = answer\n",
    "    save_json(ANSWER_CACHE_PATH, answer_cache)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query: str) -> str:\n",
    "\n",
    "    # Answer cache (no LLM cost if repeat)\n",
    "    cached = cached_answer(query)\n",
    "    if cached:\n",
    "        return cached\n",
    "\n",
    "    #  Init + ingest (ingest only first time)\n",
    "    collection = init_chroma()\n",
    "    ingest_all_txt(collection)\n",
    "\n",
    "    #  Query understanding\n",
    "    q_ctx = extract_query_context(query)\n",
    "\n",
    "    # Retrieval\n",
    "    candidates = retrieve_candidates(collection, query, q_ctx)\n",
    "    if not candidates:\n",
    "        final = \"Final Answer: No matching documents found.\\nSources: none\"\n",
    "        store_answer(query, final)\n",
    "        return final\n",
    "\n",
    "    # Rerank with conflict logic\n",
    "    ranked = rerank_with_conflict_logic(query, candidates, q_ctx, top_k=3)\n",
    "\n",
    "    # Final reasoning over multiple docs (with trimmed segments)\n",
    "    final = reason_over_docs(query, ranked, q_ctx)\n",
    "\n",
    "    # Store in cache\n",
    "    store_answer(query, final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(answer(\"I just joined as a new intern. Can I work from home?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
